{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('\\n\\n', '\\n').replace('\\n\\n', '\\n').replace('\\n\\n', '\\n')    \n",
    "    tokens = doc.split() # split into tokens by white space    \n",
    "    table = str.maketrans('', '', string.punctuation) # remove punctuation from each token\n",
    "    tokens = [w.translate(table) for w in tokens]    \n",
    "    tokens = [word for word in tokens if word.isalpha()] # remove remaining tokens that are not alphabetic    \n",
    "    tokens = [word.lower() for word in tokens] # make lower case\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agile in a Nutshell\n",
      "\n",
      "What would it take \n"
     ]
    }
   ],
   "source": [
    "open('the_agile_samurai.txt', encoding=\"utf8\").read()\n",
    "print(doc[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agile', 'in', 'a', 'nutshell', 'what', 'would', 'it', 'take', 'to', 'deliver', 'something', 'of', 'value', 'each', 'and', 'every', 'week', 'the', 'question', 'we']\n",
      "Total Tokens: 43146\n",
      "Unique Tokens: 4007\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print(\"Total Tokens: \" + str(len(tokens)))\n",
    "print(\"Unique Tokens: \" + str(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 43125\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "# organize into sequences of tokens\n",
    "length = SEQUENCE_LENGTH + 1 # the one here is the next token which is the label in our case\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "    \n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43125, 20)\n",
      "(43125, 4008)\n",
      "[  24   12    6 1211   16   73    9   72    2  139   56    5  138  173\n",
      "    3  114  225    1  446   13]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "Y = to_categorical(y, num_classes=vocab_size)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 20, 50)            200400    \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 20, 128)           68736     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4008)              517032    \n",
      "=================================================================\n",
      "Total params: 901,368\n",
      "Trainable params: 901,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "43125/43125 [==============================] - 13s 308us/step - loss: 6.4813 - acc: 0.0472\n",
      "Epoch 2/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 6.0517 - acc: 0.0667\n",
      "Epoch 3/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 5.7551 - acc: 0.0877\n",
      "Epoch 4/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 5.5317 - acc: 0.1014\n",
      "Epoch 5/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 5.3042 - acc: 0.1190\n",
      "Epoch 6/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 5.0937 - acc: 0.1333\n",
      "Epoch 7/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 4.8911 - acc: 0.1462\n",
      "Epoch 8/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 4.6832 - acc: 0.1574\n",
      "Epoch 9/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 4.4690 - acc: 0.1702\n",
      "Epoch 10/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 4.2540 - acc: 0.1885\n",
      "Epoch 11/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 4.0379 - acc: 0.2036\n",
      "Epoch 12/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 3.8256 - acc: 0.2229\n",
      "Epoch 13/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 3.6164 - acc: 0.2450\n",
      "Epoch 14/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 3.4303 - acc: 0.2683\n",
      "Epoch 15/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 3.2486 - acc: 0.2946\n",
      "Epoch 16/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 3.0754 - acc: 0.3204\n",
      "Epoch 17/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 2.9209 - acc: 0.3433\n",
      "Epoch 18/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 2.7762 - acc: 0.3646\n",
      "Epoch 19/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 2.6479 - acc: 0.3889\n",
      "Epoch 20/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 2.5180 - acc: 0.4102\n",
      "Epoch 21/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 2.4020 - acc: 0.4287\n",
      "Epoch 22/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 2.3046 - acc: 0.4485\n",
      "Epoch 23/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 2.2111 - acc: 0.4634\n",
      "Epoch 24/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 2.1199 - acc: 0.4790\n",
      "Epoch 25/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 2.0441 - acc: 0.4935\n",
      "Epoch 26/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.9609 - acc: 0.5094\n",
      "Epoch 27/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.8813 - acc: 0.5245\n",
      "Epoch 28/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.8357 - acc: 0.5320\n",
      "Epoch 29/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.7693 - acc: 0.5460\n",
      "Epoch 30/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.7083 - acc: 0.5562\n",
      "Epoch 31/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.6413 - acc: 0.5731\n",
      "Epoch 32/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.5805 - acc: 0.5848\n",
      "Epoch 33/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.5471 - acc: 0.5889\n",
      "Epoch 34/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.5002 - acc: 0.6009\n",
      "Epoch 35/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 1.4509 - acc: 0.6109\n",
      "Epoch 36/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.4205 - acc: 0.6172\n",
      "Epoch 37/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.3640 - acc: 0.6275\n",
      "Epoch 38/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.3256 - acc: 0.6392\n",
      "Epoch 39/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.2983 - acc: 0.6457\n",
      "Epoch 40/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.2541 - acc: 0.6538\n",
      "Epoch 41/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.2161 - acc: 0.6617\n",
      "Epoch 42/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.1988 - acc: 0.6618\n",
      "Epoch 43/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.1691 - acc: 0.6722\n",
      "Epoch 44/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.1418 - acc: 0.6807\n",
      "Epoch 45/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 1.1199 - acc: 0.6852\n",
      "Epoch 46/100\n",
      "43125/43125 [==============================] - 9s 218us/step - loss: 1.0937 - acc: 0.6872\n",
      "Epoch 47/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 1.0577 - acc: 0.6985\n",
      "Epoch 48/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 1.0332 - acc: 0.7024\n",
      "Epoch 49/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 1.0172 - acc: 0.7076\n",
      "Epoch 50/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.9959 - acc: 0.7138\n",
      "Epoch 51/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.9817 - acc: 0.7146\n",
      "Epoch 52/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.9504 - acc: 0.7259\n",
      "Epoch 53/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.9305 - acc: 0.7288\n",
      "Epoch 54/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.9274 - acc: 0.7293\n",
      "Epoch 55/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.9108 - acc: 0.7345\n",
      "Epoch 56/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.8927 - acc: 0.7364\n",
      "Epoch 57/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.8743 - acc: 0.7437\n",
      "Epoch 58/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.8639 - acc: 0.7444\n",
      "Epoch 59/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.8625 - acc: 0.7458\n",
      "Epoch 60/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.8375 - acc: 0.7520\n",
      "Epoch 61/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.8264 - acc: 0.7571\n",
      "Epoch 62/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.8173 - acc: 0.7565\n",
      "Epoch 63/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.8029 - acc: 0.7621\n",
      "Epoch 64/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.7869 - acc: 0.7667\n",
      "Epoch 65/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.7662 - acc: 0.7722\n",
      "Epoch 66/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.7597 - acc: 0.7729\n",
      "Epoch 67/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.7594 - acc: 0.7716\n",
      "Epoch 68/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.7439 - acc: 0.7775\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.7365 - acc: 0.7791\n",
      "Epoch 70/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.7185 - acc: 0.7828\n",
      "Epoch 71/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.7160 - acc: 0.7846\n",
      "Epoch 72/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.7112 - acc: 0.7877\n",
      "Epoch 73/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.6826 - acc: 0.7904\n",
      "Epoch 74/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.6979 - acc: 0.7905\n",
      "Epoch 75/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.6932 - acc: 0.7892\n",
      "Epoch 76/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.6758 - acc: 0.7947\n",
      "Epoch 77/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.6694 - acc: 0.7966\n",
      "Epoch 78/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.6599 - acc: 0.8013\n",
      "Epoch 79/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.6541 - acc: 0.8007\n",
      "Epoch 80/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.6414 - acc: 0.8048\n",
      "Epoch 81/100\n",
      "43125/43125 [==============================] - 10s 222us/step - loss: 0.6429 - acc: 0.8023\n",
      "Epoch 82/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.6423 - acc: 0.8043\n",
      "Epoch 83/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.6345 - acc: 0.8071\n",
      "Epoch 84/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.6287 - acc: 0.8072\n",
      "Epoch 85/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.6167 - acc: 0.8119\n",
      "Epoch 86/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.6158 - acc: 0.8110\n",
      "Epoch 87/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.6082 - acc: 0.8141\n",
      "Epoch 88/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.6076 - acc: 0.8147\n",
      "Epoch 89/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.6158 - acc: 0.8128\n",
      "Epoch 90/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.6028 - acc: 0.8151\n",
      "Epoch 91/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.5947 - acc: 0.8187\n",
      "Epoch 92/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.5868 - acc: 0.8214\n",
      "Epoch 93/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.5794 - acc: 0.8212\n",
      "Epoch 94/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.5761 - acc: 0.8240\n",
      "Epoch 95/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.5700 - acc: 0.8248\n",
      "Epoch 96/100\n",
      "43125/43125 [==============================] - 10s 221us/step - loss: 0.5674 - acc: 0.8258\n",
      "Epoch 97/100\n",
      "43125/43125 [==============================] - 10s 220us/step - loss: 0.5770 - acc: 0.8229\n",
      "Epoch 98/100\n",
      "43125/43125 [==============================] - 9s 219us/step - loss: 0.5584 - acc: 0.8274\n",
      "Epoch 99/100\n",
      "43125/43125 [==============================] - 9s 218us/step - loss: 0.5719 - acc: 0.8267\n",
      "Epoch 100/100\n",
      "43125/43125 [==============================] - 9s 220us/step - loss: 0.5627 - acc: 0.8270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fad48296c50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=SEQUENCE_LENGTH))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit(X, Y, batch_size= 256, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Sample7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index\n",
    "index2word = dict((c, w) for w, c in word2index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=SEQUENCE_LENGTH, truncating='pre')\n",
    "\n",
    "        prediction = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map predicted word index to word        \n",
    "        out_word = index2word[prediction[0]]\n",
    "      \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agile team should be fully aware that the delivery is not\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = \"agile team should be fully aware that the delivery is not\"\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about them as possible by feature set decision what do this is truly an agile coach and rockstar project manager all rolled up in one agile coaches can be very helpful in getting new teams going they can start software are always kept the faint of fourteen cookies glass of\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def saveDictionary(dict, fileName) :\n",
    "    f = open(fileName, \"w\", newline='')\n",
    "    w = csv.writer(f)\n",
    "    for key, val in dict.items():\n",
    "        w.writerow([key, val])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDictionary(word2index, \"word2index.csv\")\n",
    "saveDictionary(index2word, \"index2word.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
