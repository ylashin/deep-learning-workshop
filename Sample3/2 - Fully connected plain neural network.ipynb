{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network\n",
    "\n",
    "Our first trial would be to use Keras to develop a fully connected neural network with a single hidden layer. So we will start with the most basic layout of a neural network and see how it works for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Common python import statements\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Dense, Flatten\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# fixed random seed to have consistent results\n",
    "np.random.seed(123)\n",
    "\n",
    "# dimensions of our images. Keras will be resizing all images to have consistent data fed to training and testing phases\n",
    "img_width, img_height = 300, 300 \n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/test'\n",
    "nb_train_samples = 3000\n",
    "nb_validation_samples = 300\n",
    "epochs = 5\n",
    "batch_size = 400 # in next steps we cannot just use any size here as it will be limited by GPU memory size.\n",
    "# if you get OOM exception anytime, decrease this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need do define input data format or shape. Tensorflow defaults for image are height then width then colour channels\n",
    "# height and width are the same here but can have some implications when they are different and when doing prediction (inference)\n",
    "\n",
    "input_shape = (img_height, img_width, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # sequential mode is a stack of layers one after other and this is the simplest network layout\n",
    "model.add(Dense(3, input_shape=input_shape)) # this is input layer apparently\n",
    "model.add(Flatten()) # flatten the multi dimensional input into a single dimension (vector)\n",
    "model.add(Dense(32, activation = 'relu')) # RELU is simply max(x,0) which adds some non linear activation\n",
    "model.add(Dense(1, activation = 'sigmoid')) # softmax is used in output layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300, 300, 3)       12        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 270000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8640032   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,640,077\n",
      "Trainable params: 8,640,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you see above that a simple network for 300x300 image has more than 8 million weights to train. Totally crazy !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any model we need to compile it first with a loss function based on the problem and an optimizer and a performance metric.\n",
    "I can just throw two words on accuracy here. If we are doing dogs vs cats then we may be concerned with \n",
    "total number of correct predictions only. But what if we are doing cancer detection where the percentage of \n",
    "positives is something like 1%. Clearly we do not want to inform someone you have cancer when he does not and also\n",
    "we do not want to miss people with cancer and let them go home without informing them. So for such cases there are other \n",
    "metrics like recall and precision. Just google \"F1 Score\" for more insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to prepare some generators that can feed Keras with data for training and testing. They shield us from the hard task of loading images and converting them to multi dimensional arrays and so on. Also sometimes training datasets could be small so those generators can do some process called data augmentation to fake new training data. For example, images could be rotated a bit or flipped as long as they contain the same main patterns needed for training. \n",
    "\n",
    "*Side note:*\n",
    "Scaling to 1/255 below is very common in many vision problems and appears in most of preprocessing steps of major commonly used models like VGG, Inception & RESNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training, some scaling and image manipulation as well\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing, only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step we will train the network and adjusting the weights. Output of final training epoch will be final accuracy for the few epochs run. An epoch is a training run against all training data that is spearated into a number of steps in which each step will be training a specific number of training samples of size (batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 65s 8s/step - loss: 7.0299 - acc: 0.4930 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 63s 8s/step - loss: 8.1128 - acc: 0.4967 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 60s 8s/step - loss: 8.0859 - acc: 0.4983 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 60s 8s/step - loss: 8.0501 - acc: 0.5006 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 60s 8s/step - loss: 8.0367 - acc: 0.5014 - val_loss: 8.0590 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f48a26d8518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=math.ceil(nb_train_samples / batch_size),\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=math.ceil(nb_validation_samples/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a very bad result, you know why? A binary classification problem with a dummy solver will give 50% accuracy if the solver picks one of the two classes all the time assuming the dataset is balanced.\n",
    "Let's see in notebook #3 if we can do better by adding another hidden layer or maybe more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
